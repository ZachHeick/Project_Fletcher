{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic: Project 4   \n",
    "Subject: Getting and Cleaning Reddit Comments by Subreddit  \n",
    "Date: 11/10/2017   \n",
    "Name: Zach Heick   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**: For each subreddit, I cleaned the comments and recorded metadata for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clean_comments(subreddit):\n",
    "    \"\"\"\n",
    "    Connects to a subreddit collection in a MongoDB hosted on AWS.\n",
    "    For each comment:\n",
    "        -markdown syntax and hyperlinks are removed\n",
    "        -metadata for each comment is recorded\n",
    "        -words are lemmatized and stop words are removed\n",
    "    :param subreddit: name of the subreddit\n",
    "    :returns: list of dictionaries for each comment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Connect to MongoDB and get raw comments\n",
    "    MONGO_U = os.environ['MONGO_U']\n",
    "    MONGO_P = os.environ['MONGO_P']\n",
    "    \n",
    "    client = MongoClient('mongodb://' + MONGO_U + ':' + MONGO_P + '@13.59.55.238/reddit_comments_db')\n",
    "    db = client['reddit_comments_db']\n",
    "    collection_name = '{}_comments_collection'.format(subreddit)\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    raw_comments = []\n",
    "    for d in collection.find():\n",
    "        raw_comments += d['comments']\n",
    "        \n",
    "    _stopwords = stopwords.words()\n",
    "    _lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    url_regex_markdown = \"http[s]?://[^)]+\"\n",
    "    url_regex = \"http\\S+\"\n",
    "\n",
    "    comments = []\n",
    "    \n",
    "    for comment_tuple in raw_comments:\n",
    "        raw_comment = comment_tuple[0]\n",
    "        score = comment_tuple[1]\n",
    "        time = comment_tuple[2]\n",
    "\n",
    "        comment_d = {}\n",
    "        \n",
    "        # Remove markdown syntax and hyperlinks. Do not include any deleted or removed comments.\n",
    "        if '>' not in raw_comment and raw_comment not in ['.', '[deleted]', '[removed]']:              \n",
    "            comment_no_markdown = re.sub(r'\\(\\s*({})\\s*\\)'.format(url_regex_markdown), ' ', raw_comment) \\\n",
    "                .replace('*', '') \\\n",
    "                .replace('&nbsp;', '') \\\n",
    "                .replace('[',' ').replace(']',' ') \\\n",
    "                .replace('(', ' ').replace(')', ' ')\n",
    "            cleaned_comment = re.sub(r'{}'.format(url_regex), ' ', comment_no_markdown, flags=re.MULTILINE) \\\n",
    "                .replace('\\n', ' ')\n",
    "\n",
    "            # Record the polarity and subjectivity of each sentence in the comment\n",
    "            if cleaned_comment.strip() != '' and 'I am a bot' not in cleaned_comment:\n",
    "                subjectivities = [0]\n",
    "                polarities = [0]\n",
    "                for split in re.split('[?:!.]', cleaned_comment):\n",
    "                    sentiment = TextBlob(split).sentiment\n",
    "                    if split != '':\n",
    "                        subjectivities.append(float('{0:.4f}'.format(sentiment.subjectivity)))\n",
    "                        polarities.append(float('{0:.4f}'.format(sentiment.polarity)))\n",
    "\n",
    "                # Comment metadata\n",
    "                sentiment = TextBlob(cleaned_comment).sentiment\n",
    "                comment_d['max_subjectivity'] = max(subjectivities)\n",
    "                comment_d['min_subjectivity'] = min(subjectivities)\n",
    "                comment_d['max_polarity'] = max(polarities)\n",
    "                comment_d['min_polarity'] = min(polarities)\n",
    "                comment_d['overall_polarity'] = float('{0:.4f}'.format(sentiment.polarity))\n",
    "\n",
    "                words = cleaned_comment.split()\n",
    "\n",
    "                comment_d['words_count'] = len(words)\n",
    "                comment_d['char_count'] = len(cleaned_comment.strip())\n",
    "                comment_d['time'] = float('{0:.2f}'.format(time))\n",
    "                comment_d['score'] = score\n",
    "\n",
    "                # Lemmatize the words and remove stopwords from the comment.\n",
    "                cleaned_words = []\n",
    "                for word in words:\n",
    "                    lemm_word = _lemmatizer.lemmatize(word)\n",
    "                    if lemm_word not in _stopwords:\n",
    "                        cleaned_words.append(lemm_word)\n",
    "                comment_d['comment'] = ' '.join(cleaned_words)\n",
    "                comments.append(comment_d)\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddits = ['politics', 'atheism', 'hiphopheads', 'science', 'worldnews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for subreddit in subreddits:\n",
    "    with open('{}_comments.pickle'.format(subreddit), 'wb') as f:\n",
    "        comments = get_clean_comments(subreddit)\n",
    "        pickle.dump(comments, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('subreddits.pickle', 'wb') as f:\n",
    "    pickle.dump(subreddits, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
